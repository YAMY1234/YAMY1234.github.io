<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>graduate training one | Have you seen my cat?</title><meta name="keywords" content="deep_learning"><meta name="author" content="YAMY"><meta name="copyright" content="YAMY"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="毕业实训-1服务器密码123454 多模态相关性研究Bi-Bimodal Modality Fusion for Correlation-Controlled Multimodal Sentiment AnalysisAbstract 多模态情感分析旨在提取和整合从多种模态中收集的语义信息，以识别多模态数据中表达的情绪和情绪。该研究领域的主要关注点在于开发一种特殊的融合方案，可以从各种模式中提取和">
<meta property="og:type" content="article">
<meta property="og:title" content="graduate training one">
<meta property="og:url" content="http://yamy1234.github.io/2022/10/30/graduate-training-one/index.html">
<meta property="og:site_name" content="Have you seen my cat?">
<meta property="og:description" content="毕业实训-1服务器密码123454 多模态相关性研究Bi-Bimodal Modality Fusion for Correlation-Controlled Multimodal Sentiment AnalysisAbstract 多模态情感分析旨在提取和整合从多种模态中收集的语义信息，以识别多模态数据中表达的情绪和情绪。该研究领域的主要关注点在于开发一种特殊的融合方案，可以从各种模式中提取和">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yamy1234.github.io/img/5.jpg">
<meta property="article:published_time" content="2022-10-30T13:06:36.000Z">
<meta property="article:modified_time" content="2022-10-31T10:05:50.005Z">
<meta property="article:author" content="YAMY">
<meta property="article:tag" content="deep_learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yamy1234.github.io/img/5.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://yamy1234.github.io/2022/10/30/graduate-training-one/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'graduate training one',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-10-31 18:05:50'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.1.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/head.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">18</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">6</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">7</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/5.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Have you seen my cat?</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">graduate training one</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-10-30T13:06:36.000Z" title="Created 2022-10-30 21:06:36">2022-10-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2022-10-31T10:05:50.005Z" title="Updated 2022-10-31 18:05:50">2022-10-31</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/graduate-training/">graduate_training</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/graduate-training/deep-learning/">deep_learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="graduate training one"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="毕业实训-1"><a href="#毕业实训-1" class="headerlink" title="毕业实训-1"></a>毕业实训-1</h1><h2 id="服务器密码"><a href="#服务器密码" class="headerlink" title="服务器密码"></a>服务器密码</h2><p>123454</p>
<h2 id="多模态相关性研究"><a href="#多模态相关性研究" class="headerlink" title="多模态相关性研究"></a>多模态相关性研究</h2><h3 id="Bi-Bimodal-Modality-Fusion-for-Correlation-Controlled-Multimodal-Sentiment-Analysis"><a href="#Bi-Bimodal-Modality-Fusion-for-Correlation-Controlled-Multimodal-Sentiment-Analysis" class="headerlink" title="Bi-Bimodal Modality Fusion for Correlation-Controlled Multimodal Sentiment Analysis"></a>Bi-Bimodal Modality Fusion for Correlation-Controlled Multimodal Sentiment Analysis</h3><p>Abstract</p>
<p>多模态情感分析旨在提取和整合从多种模态中收集的语义信息，以识别多模态数据中表达的情绪和情绪。该研究领域的主要关注点在于开发一种特殊的融合方案，可以从各种模式中提取和整合关键信息。然而，一个可能限制先前工作达到更高水平的问题是缺乏对模态之间的独立性和相关性之间的动态竞争的适当建模，这可能会导致模态特定特征空间的崩溃或引入额外的噪声，从而恶化融合结果。为了缓解这一问题，我们提出了双双峰融合网络(BBFN)，这是一种新颖的端到端网络，在成对模态表示上执行融合(相关增量)和分离(差异增量)。两个部分同时训练，从而模拟它们之间的战斗。由于模态之间已知的信息不平衡，该模型以两个双峰对作为输入。此外，我们利用Transformer体系结构中的门控机制来进一步改进最终输出。在三个数据集(CMU-MOSI, CMU-MOSEI和UR-FUNNY)上的实验结果验证了我们的模型明显优于SOTA。该工作的实现可在<a target="_blank" rel="noopener" href="https://github.com/declare-lab/multimodal-deep-learning%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/declare-lab/multimodal-deep-learning获得。</a></p>
<p>Code link</p>
<p><a target="_blank" rel="noopener" href="https://github.com/declare-lab/multimodal-deep-learning">https://github.com/declare-lab/multimodal-deep-learning</a></p>
<h2 id="对于Multimodal-Sentiment-Analysis的整体分析"><a href="#对于Multimodal-Sentiment-Analysis的整体分析" class="headerlink" title="对于Multimodal Sentiment Analysis的整体分析"></a>对于Multimodal Sentiment Analysis的整体分析</h2><table>
<thead>
<tr>
<th>Dataset</th>
<th>Best Model</th>
<th>Paper</th>
<th>Code</th>
<th>Compare</th>
</tr>
</thead>
<tbody><tr>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/sota/multimodal-sentiment-analysis-on-mosi">MOSI</a></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/sota/multimodal-sentiment-analysis-on-mosi"> CM-BERT</a></td>
<td></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/sota/multimodal-sentiment-analysis-on-mosi">See all</a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/sota/multimodal-sentiment-analysis-on-cmu-mosei-1">CMU-MOSEI</a></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/sota/multimodal-sentiment-analysis-on-cmu-mosei-1"> MMLatch</a></td>
<td></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/sota/multimodal-sentiment-analysis-on-cmu-mosei-1">See all</a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/sota/multimodal-sentiment-analysis-on-cmu-mosi">CMU-MOSI</a></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/sota/multimodal-sentiment-analysis-on-cmu-mosi"> TEASEL</a></td>
<td></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/sota/multimodal-sentiment-analysis-on-cmu-mosi">See all</a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/sota/multimodal-sentiment-analysis-on-b-t4sa">B-T4SA</a></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/sota/multimodal-sentiment-analysis-on-b-t4sa"> AutoML-Based Fusion Approach</a></td>
<td></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://paperswithcode.com/sota/multimodal-sentiment-analysis-on-b-t4sa">See all</a></td>
</tr>
</tbody></table>
<h2 id="模态融合——交互建模"><a href="#模态融合——交互建模" class="headerlink" title="模态融合——交互建模"></a>模态融合——交互建模</h2><p>模态融合相关研究主要侧重于用基于Transformer、Bert等多模态编码器对图像和文本特征之间的交互进行建模；也包括结合传统、前缀等其他创兴的方式进行模态融合的一些研究。</p>
<h3 id="Multimodal-Sentiment-Analysis-with-Word-Level-Fusion-and-Reinforcement-Learning"><a href="#Multimodal-Sentiment-Analysis-with-Word-Level-Fusion-and-Reinforcement-Learning" class="headerlink" title="Multimodal Sentiment Analysis with Word-Level Fusion and Reinforcement Learning"></a>Multimodal Sentiment Analysis with Word-Level Fusion and Reinforcement Learning</h3><p><a target="_blank" rel="noopener" href="https://github.com/pliang279/MFN"> pliang279&#x2F;MFN</a></p>
<h3 id="Multimodal-Transformer-for-Unaligned-Multimodal-Language-Sequences"><a href="#Multimodal-Transformer-for-Unaligned-Multimodal-Language-Sequences" class="headerlink" title="Multimodal Transformer for Unaligned Multimodal Language Sequences"></a>Multimodal Transformer for Unaligned Multimodal Language Sequences</h3><h3 id="Improving-Multimodal-Fusion-with-Hierarchical-Mutual-Information-Maximization-for-Multimodal-Sentiment-Analysis"><a href="#Improving-Multimodal-Fusion-with-Hierarchical-Mutual-Information-Maximization-for-Multimodal-Sentiment-Analysis" class="headerlink" title="Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis"></a>Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis</h3><p><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/improving-multimodal-fusion-with-hierarchical">https://paperswithcode.com/paper/improving-multimodal-fusion-with-hierarchical</a></p>
<p>在多模态情感分析(MSA)中，模型的性能在很大程度上取决于合成嵌入的质量。这些嵌入是由称为多模态融合的上游过程生成的，该过程旨在提取和组合输入的单模态原始数据，以产生更丰富的多模态表示。以前的工作要么反向传播任务丢失，要么操纵特征空间的几何特性来产生良好的融合结果，这忽略了从输入到融合结果的关键任务相关信息的保存。本文提出了一种多模态信息max (MultiModal InfoMax, MMIM)框架，该框架分层最大化单模态输入对(intermodal)和多模态融合结果与单模态输入之间的互信息(MI)，通过多模态融合来维护任务相关信息。框架与主任务(MSA)联合训练，以提高下游MSA任务的性能。为了解决MI界的棘手问题，我们进一步制定了一套计算简单的参数和非参数方法来近似它们的真值。在两个广泛使用的数据集上的实验结果证明了我们的方法的有效性。该工作的实现可在<a target="_blank" rel="noopener" href="https://github.com/declare-lab/Multimodal-Infomax%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/declare-lab/Multimodal-Infomax公开获取。</a></p>
<h3 id="Cross-Modal-BERT-for-Text-Audio-Sentiment-Analysis"><a href="#Cross-Modal-BERT-for-Text-Audio-Sentiment-Analysis" class="headerlink" title="Cross-Modal BERT for Text-Audio Sentiment Analysis"></a>Cross-Modal BERT for Text-Audio Sentiment Analysis</h3><p>跨模态的bert</p>
<p><a target="_blank" rel="noopener" href="https://github.com/thuiar/Cross-Modal-BERT">https://github.com/thuiar/Cross-Modal-BERT</a></p>
<h3 id="Multimodal-Emotion-Recognition-with-Transformer-Based-Self-Supervised-Feature-Fusion"><a href="#Multimodal-Emotion-Recognition-with-Transformer-Based-Self-Supervised-Feature-Fusion" class="headerlink" title="Multimodal Emotion Recognition with Transformer-Based Self Supervised Feature Fusion"></a>Multimodal Emotion Recognition with Transformer-Based Self Supervised Feature Fusion</h3><p>由于情感识别的复杂性，它是一个具有挑战性的研究领域，人类通过各种方式表达情感线索，如语言、面部表情和讲话。特征的表征和融合是多模态情感识别研究的关键。自我监督学习(SSL)已经成为表征学习中一个重要的和有影响力的研究方向，研究人员可以访问预先训练的代表不同数据模式的SSL模型。在文献中，我们第一次用从独立预先训练的SSL模型中提取的特征来表示文本、音频(语音)和视觉的三种输入模式。鉴于SSL特征的高维特性，我们引入了一种新的变形器和基于注意的融合机制，该机制可以结合多模态SSL特征，并在多模态情感识别任务中实现最先进的结果。我们对我们的工作进行了基准测试和评估，以表明我们的模型是稳健的，并且在四个数据集上优于最先进的模型。</p>
<p>自监督的transformer，有点意思</p>
<p><a target="_blank" rel="noopener" href="https://github.com/shamanez/Self-Supervised-Embedding-Fusion-Transformer">https://github.com/shamanez/Self-Supervised-Embedding-Fusion-Transformer</a></p>
<h3 id="SWAFN-Sentimental-Words-Aware-Fusion-Network-for-Multimodal-Sentiment-Analysis"><a href="#SWAFN-Sentimental-Words-Aware-Fusion-Network-for-Multimodal-Sentiment-Analysis" class="headerlink" title="SWAFN: Sentimental Words Aware Fusion Network for Multimodal Sentiment Analysis"></a>SWAFN: Sentimental Words Aware Fusion Network for Multimodal Sentiment Analysis</h3><p>语言知识，浅层、深层；感觉一般；只是改善了融合表征。</p>
<p>Multimodal sentiment analysis aims to predict sentiment of language text with the help of other modalities, such as vision and acoustic features. Previous studies focused on learning the joint representation of multiple modalities, ignoring some useful knowledge contained in language modal. In this paper, we try to incorporate sentimental words knowledge into the fusion network to guide the learning of joint representation of multimodal features. Our method consists of two components: shallow fusion part and aggregation part. For the shallow fusion part, we use crossmodal coattention mechanism to obtain bidirectional context information of each two modals to get the fused shallow representations. For the aggregation part, we design a multitask of sentimental words classification to help and guide the deep fusion of the three modalities and obtain the final sentimental words aware fusion representation. We carry out several experiments on CMU-MOSI, CMU-MOSEI and YouTube datasets. The experimental results show that introducing sentimental words prediction as a multitask can really improve the fusion representation of multiple modalities.</p>
<h3 id="TEASEL-A-Transformer-Based-Speech-Prefixed-Language-Model"><a href="#TEASEL-A-Transformer-Based-Speech-Prefixed-Language-Model" class="headerlink" title="TEASEL: A Transformer-Based Speech-Prefixed Language Model"></a>TEASEL: A Transformer-Based Speech-Prefixed Language Model</h3><p>多模态语言分析是NLP的一个新兴领域，旨在同时对说话者的话语、声学注释和面部表情进行建模。在这方面，词汇特征通常优于其他模式，因为它们是通过基于transformer的模型在大型语料库上预先训练的。尽管它们的性能很好，但由于数据不足，在任何模态下训练一个新的自监督学习(SSL) Transformer通常都是无法实现的，这就是多模态语言学习的情况。本研究提出了一种基于Transformer的语音前缀语言模型(testel)来处理上述约束，而无需训练一个完整的Transformer模型。与传统语言模型相比，TEASEL模型除了包含语篇情态外，还包含了语音情态作为动态前缀。该方法利用传统的预训练语言模型作为跨模态Transformer模型。我们对基于CMU-MOSI数据集定义的多模态情感分析任务的testel进行了评估。大量的实验表明，我们的模型在f1得分上比单模态基线语言模型高出4%，比目前的多模态最先进(SoTA)模型高出1%。此外，我们提出的方法比SoTA模型小72%。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/tjdevWorks/TEASEL">https://github.com/tjdevWorks/TEASEL</a></p>
<p>这也是一个自监督学习的transformer</p>
<h3 id="An-AutoML-based-Approach-to-Multimodal-Image-Sentiment-Analysis"><a href="#An-AutoML-based-Approach-to-Multimodal-Image-Sentiment-Analysis" class="headerlink" title="An AutoML-based Approach to Multimodal Image Sentiment Analysis"></a>An AutoML-based Approach to Multimodal Image Sentiment Analysis</h3><p>计算方式和svm类似，并且成果只比SVM高了0.03的百分点，不具备参考意义。</p>
<p>情感分析是一种专注于分析数据以提取与情绪相关的信息的研究课题。情绪分析的应用范围很广，从推荐系统、市场营销到客户满意度。最近的方法是使用机器学习技术来评估文本内容，这些技术是在大型语料库中训练的。然而，随着社交媒体的发展，其他数据类型大量出现，比如图像。图像中的情感分析已被证明是文本数据的一个有价值的补充，因为它能够通过创建上下文和连接推断潜在的信息极性。多模态情感分析方法意图利用文本和图像内容的信息来执行评价。尽管最近取得了一些进展，但目前的解决方案在结合图像和文本信息对社交媒体数据进行分类方面仍然存在困难，这主要是由于主观性、类间同质性和融合数据差异。在本文中，我们提出了一种基于AutoML的融合分类方法，将文本和图像个体情感分析结合起来，并进行随机搜索以找到最佳模型。我们的方法在B-T4SA数据集上取得了最先进的性能，准确率为95.19%。</p>
<h3 id="MMLatch-Bottom-up-Top-down-Fusion-for-Multimodal-Sentiment-Analysis"><a href="#MMLatch-Bottom-up-Top-down-Fusion-for-Multimodal-Sentiment-Analysis" class="headerlink" title="MMLatch: Bottom-up Top-down Fusion for Multimodal Sentiment Analysis"></a>MMLatch: Bottom-up Top-down Fusion for Multimodal Sentiment Analysis</h3><p>不同表征-如自上而下表征的影响</p>
<p>目前用于多模态融合的深度学习方法依赖于高、中水平潜在模态表征(后期&#x2F;中期融合)或低水平感觉输入(早期融合)的自底向上融合。人类感知模型强调了自上而下融合的重要性，在这种融合中，高级表征影响感知输入的方式，即认知影响感知。目前的深度学习模型没有捕捉到这些自上而下的交互作用。在这项工作中，我们提出了一种神经体系结构，它可以捕获自顶向下的跨模态交互，在网络训练过程中使用前向传递的反馈机制。提出的机制为每个模态提取高级表示，并使用这些表示来屏蔽感官输入，允许模型执行自顶向下的特征屏蔽。将该模型应用于CMU-MOSEI多模态情感识别。我们的方法在完善的MulT和强大的晚期融合基线上显示了一致的改进，取得了最先进的结果。</p>
<h2 id="模态转化——其他模态到【语言】模态的迁移-x2F-映射-x2F-辅助"><a href="#模态转化——其他模态到【语言】模态的迁移-x2F-映射-x2F-辅助" class="headerlink" title="模态转化——其他模态到【语言】模态的迁移&#x2F;映射&#x2F;辅助"></a>模态转化——其他模态到【语言】模态的迁移&#x2F;映射&#x2F;辅助</h2><p>对于模态转化来说，我的定义是模态之间的重量级并非对等，而是以其中一个为中心，另外的模态作为中心模态进行迁移、映射、辅助补充等。</p>
<h3 id="Words-Can-Shift-Dynamically-Adjusting-Word-Representations-Using-Nonverbal-Behaviors"><a href="#Words-Can-Shift-Dynamically-Adjusting-Word-Representations-Using-Nonverbal-Behaviors" class="headerlink" title="Words Can Shift: Dynamically Adjusting Word Representations Using Nonverbal Behaviors"></a>Words Can Shift: Dynamically Adjusting Word Representations Using Nonverbal Behaviors</h3><p>在面对面的交流中，人们通过使用语言和非语言行为来传达他们的意图。说话者的意图通常根据不同的非语言环境动态变化，如声音模式和面部表情。因此，在建模人类语言时，不仅要考虑词语的字面意义，还要考虑这些词语出现的非语言语境。为了更好地建模人类语言，我们首先通过分析词段中出现的细粒度视觉和听觉模式来建模有表现力的非语言表征。此外，我们试图通过基于伴随的非语言行为的词汇表征来捕捉非语言意图的动态本质。为此，我们提出了循环参与变异嵌入网络(RAVEN)，它模拟非语言子词序列的细粒度结构，并基于非语言线索动态地转移词表示。我们提出的模型在两个公开的多模态情感分析和情感识别数据集上实现了具有竞争力的性能。我们还可视化了在不同的非语言语境中词汇表征的变化，并总结了词汇表征的多模态变化的共同模式。</p>
<p>link：<a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/words-can-shift-dynamically-adjusting-word">https://paperswithcode.com/paper/words-can-shift-dynamically-adjusting-word</a></p>
<h3 id="Multimodal-Sentiment-Analysis-using-Hierarchical-Fusion-with-Context-Modeling"><a href="#Multimodal-Sentiment-Analysis-using-Hierarchical-Fusion-with-Context-Modeling" class="headerlink" title="Multimodal  Sentiment Analysis using Hierarchical Fusion with Context Modeling"></a>Multimodal  Sentiment Analysis using Hierarchical Fusion with Context Modeling</h3><p>多模态情感分析是一个非常积极发展的研究领域。改进多模态融合机制是该领域的一个有前途的领域。我们提出了一种新的特征融合策略，它以分层的方式进行，首先将两种模式融合，然后才将所有三种模式融合。在个体话语的多模态情感分析上，我们的策略比传统的特征拼接方法高出1%，错误率降低了5%。在多话语视频剪辑的话语级多模态情感分析中，目前最先进的技术将来自同一片段其他话语的上下文信息整合在了一起，我们的层次融合比目前使用的串联算法高出2.4%(错误率降低近10%)。我们的方法的实现以开源代码的形式公开提供。</p>
<p>感觉这个比较一般哈，主要是这个东西很奇怪的地方在于github上面的评价只有3star</p>
<p><a target="_blank" rel="noopener" href="https://github.com/SenticNet/hfusion">https://github.com/SenticNet/hfusion</a></p>
<h3 id="Exploiting-BERT-For-Multimodal-Target-Sentiment-Classification-Through-Input-Space-Translation"><a href="#Exploiting-BERT-For-Multimodal-Target-Sentiment-Classification-Through-Input-Space-Translation" class="headerlink" title="Exploiting BERT For Multimodal Target Sentiment Classification Through Input Space Translation"></a>Exploiting BERT For Multimodal Target Sentiment Classification Through Input Space Translation</h3><p>翻译图像作为辅助语句，对于bert提供信息</p>
<p>多模态目标&#x2F;方面情感分类结合了多模态情感分析和方面&#x2F;目标情感分类。该任务的目标是结合视觉和语言来理解句子中对目标实体的情感。Twitter是完成这项任务的理想环境，因为它本身是多模式的、高度情绪化的，并且影响现实世界的事件。然而，多模式推文很短，并伴随着复杂的，可能无关的图像。我们引入了一种双流模型，该模型使用对象感知转换器和单遍非自回归文本生成方法在输入空间中翻译图像。然后，我们利用翻译构建一个辅助句，为语言模型提供多模态信息。我们的方法增加了语言模型可用的文本数量，并从复杂图像中提取出对象级信息。我们在两个多模态Twitter数据集上实现了最先进的性能，而无需修改语言模型的内部结构来接受多模态数据，证明了我们的翻译的有效性。此外，我们解释了一种用于方面情感分析的流行方法在应用于tweet时的失败模式。我们的代码可在\textcolor{blue}{\url{<a target="_blank" rel="noopener" href="https://github.com/codezakh/exploiting-BERT-thru-translation%7D%7D%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/codezakh/exploiting-BERT-thru-translation}}获得。</a></p>
<h2 id="模态对比——以模态之间的相似性vs差异性来分析情态"><a href="#模态对比——以模态之间的相似性vs差异性来分析情态" class="headerlink" title="模态对比——以模态之间的相似性vs差异性来分析情态"></a>模态对比——以模态之间的相似性vs差异性来分析情态</h2><h3 id="MISA-双模态：两个不同的映射空间"><a href="#MISA-双模态：两个不同的映射空间" class="headerlink" title="MISA 双模态：两个不同的映射空间"></a>MISA 双模态：两个不同的映射空间</h3><p>多模态情感分析是一个活跃的研究领域，它利用多模态信号对用户生成的视频进行情感理解。解决这一问题的主要方法是开发复杂的核聚变技术。然而，信号的异构性质造成了分布模态差距，这构成了重大挑战。在这篇论文中，我们的目标是学习有效的模态表示来帮助融合过程。我们提出了一个新的框架，MISA，它将每个模态投射到两个不同的子空间。第一个子空间是模态不变的，其中跨模态的表示了解它们的共性并减少模态差距。第二个子空间是特定于模式的，它是每个模式的私有空间，并捕获它们的特征特征。这些表示提供了多模态数据的整体视图，用于导致任务预测的融合。我们在流行情绪分析基准MOSI和MOSEI上的实验表明，与最先进的模型相比，取得了显著的进步。我们还考虑了多模态幽默检测的任务，并在最近提出的UR_FUNNY数据集上进行了实验。在这方面，我们的模型比强基线更好，将MISA建立为一个有用的多模式框架。</p>
<h3 id="Learning-Modality-Specific-Representations-with-Self-Supervised-Multi-Task-Learning-for-Multimodal-Sentiment-Analysis"><a href="#Learning-Modality-Specific-Representations-with-Self-Supervised-Multi-Task-Learning-for-Multimodal-Sentiment-Analysis" class="headerlink" title="Learning Modality-Specific Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis"></a>Learning Modality-Specific Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis</h3><p>表征学习是多模态学习中的一项重要而富有挑战性的任务。有效的情态表征应包含两部分特征:一致性和差异性。由于统一的多模态标注，现有的方法在获取差异信息方面受到限制。但是，额外的单模态注释需要大量的时间和人力成本。在本文中，我们设计了一个基于自监督学习策略的标签生成模块来获得独立的单模态监督。然后，联合训练多模态和单模态任务，分别学习其一致性和差异性。此外，在训练阶段，我们设计了权重调整策略来平衡不同子任务之间的学习进度。即引导子任务集中在模态监督差异较大的样本上。最后，我们在三个公共多模态基线数据集上进行了广泛的实验。实验结果验证了自动生成单模态监控系统的可靠性和稳定性。在MOSI和MOSEI数据集上，我们的方法超过了目前最先进的方法。在SIMS数据集上，我们的方法实现了与人工注释单峰标签相当的性能。完整的代码可以在<a target="_blank" rel="noopener" href="https://github.com/thuiar/Self-MM%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/thuiar/Self-MM上找到。</a></p>
<h2 id="时序-对齐创新"><a href="#时序-对齐创新" class="headerlink" title="时序-对齐创新"></a>时序-对齐创新</h2><h3 id="Deep-HOSeq-Deep-Higher-Order-Sequence-Fusion-for-Multimodal-Sentiment-Analysis"><a href="#Deep-HOSeq-Deep-Higher-Order-Sequence-Fusion-for-Multimodal-Sentiment-Analysis" class="headerlink" title="Deep-HOSeq: Deep Higher Order Sequence Fusion for Multimodal Sentiment Analysis"></a>Deep-HOSeq: Deep Higher Order Sequence Fusion for Multimodal Sentiment Analysis</h3><p>多模态情感分析利用多种异构模态进行情感分类。最近的多模态融合方案通过定制lstm来发现模态内动态，并设计复杂的注意机制来从多模态序列中发现模态间动态。尽管这些方案功能强大，但它们完全依赖于注意机制，这存在两个主要缺点:1)欺骗性注意面具，2)训练动态。然而，优化这些整合体系结构的超参数，特别是它们受注意方案约束的自定义lstm需要付出很大的努力。在本研究中，我们首先利用基本LSTMs和基于张量的卷积网络，提出了一个公共网络来发现模态内和模态间动力学。然后，我们提出了独特的网络来封装模式之间的时间粒度，这是在异步序列中提取信息时必不可少的。然后，我们通过一个融合层将这两种信息集成起来，并将我们的新多模态融合方案称为Deep- hoseq(具有高阶公共和唯一序列信息的深度网络)。提出的Deep-HOSeq能有效地从多模态序列中发现所有重要的信息，并在CMU-MOSEI和CMU-MOSI基准数据集上实证证明了利用这两类信息的有效性。我们提议的Deep-HOSeq的源代码在<a target="_blank" rel="noopener" href="https://github.com/sverma88/Deep-HOSeq--ICDM-2020%E4%B8%8A%E3%80%82">https://github.com/sverma88/Deep-HOSeq--ICDM-2020上。</a></p>
<p>–我暂时不加pdf了。需要用到的话在加吧。。</p>
<h3 id="MTAG-Modal-Temporal-Attention-Graph-for-Unaligned-Human-Multimodal-Language-Sequences"><a href="#MTAG-Modal-Temporal-Attention-Graph-for-Unaligned-Human-Multimodal-Language-Sequences" class="headerlink" title="MTAG: Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences"></a>MTAG: Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences</h3><p>–注意力图</p>
<p>人类交流本质上是多模式的;人们通过语言、声音、面部表情等多种形式来表达意见和情感。该领域的数据表现出复杂的多关系和时间交互作用。从这些数据中学习是一个具有根本性挑战性的研究问题。本文提出了模态-时间注意图(MTAG)。MTAG是一种可解释的基于图的神经模型，它为分析多模态序列数据提供了一个合适的框架。我们首先介绍了一种将未对齐的多模态序列数据转换为具有异构节点和边缘的图的过程，该图捕获了跨模态和时间的丰富交互。然后，设计了一种称为MTAG融合的新型图融合操作，以及动态修剪和读出技术，以有效地处理该模态-时间图并捕获各种交互。通过学习只关注图中重要的交互，MTAG在多模态情感分析和情感识别基准测试中实现了最先进的性能，同时使用了显著较少的模型参数。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/jedyang97/MTAG">https://github.com/jedyang97/MTAG</a></p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="M-SENA-一个多模态情感分析平台？"><a href="#M-SENA-一个多模态情感分析平台？" class="headerlink" title="M-SENA: 一个多模态情感分析平台？"></a>M-SENA: 一个多模态情感分析平台？</h3><p><a target="_blank" rel="noopener" href="https://github.com/thuiar/MMSA"> 这个家伙说是提供了一个框架呢：      thuiar&#x2F;MMSA  official</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/thuiar/mmsa-fet"> thuiar&#x2F;mmsa-fet  official</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/thuiar/M-SENA"> thuiar&#x2F;M-SENA  official</a></p>
<p>自监督-学习单模态-多模态一致性+差异性</p>
<h3 id="Integrating-Multimodal-Information-in-Large-Pretrained-Transformers"><a href="#Integrating-Multimodal-Information-in-Large-Pretrained-Transformers" class="headerlink" title="Integrating Multimodal Information in Large Pretrained Transformers"></a>Integrating Multimodal Information in Large Pretrained Transformers</h3><p>最近的基于transformer的上下文单词表示，包括BERT和XLNet，在NLP的多个学科中显示了最先进的性能。对特定于任务的数据集上训练过的上下文模型进行微调是实现下游卓越性能的关键。虽然对词汇应用程序(只有语言模态的应用程序)微调这些预先训练的模型很简单，但对多模态语言(NLP中一个日益增长的领域，专注于建模面对面的交流)来说，这并不简单。预先训练的模型没有必要的组件来接受视觉和听觉两种额外的模式。在本文中，我们提出了一个附加的BERT和XLNet称为多模态适应门(MAG)。MAG允许BERT和XLNet在微调过程中接受多模态非语言数据。它通过生成转换到BERT和XLNet的内部表示来实现这一点;以视觉和听觉形式为条件的转变。在实验中，我们研究了常用的用于多模态情感分析的CMU-MOSI和CMU-MOSEI数据集。与之前的基线相比，微调MAG-BERT和MAG-XLNet显著提高了情绪分析性能，以及仅针对语言的微调BERT和XLNet。在CMU-MOSI数据集上，MAG-XLNet在NLP领域首次实现了人类水平的多模态情感分析性能。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/WasifurRahman/BERT_multimodal_transformer">https://github.com/WasifurRahman/BERT_multimodal_transformer</a></p>
<h3 id="https-paperswithcode-com-paper-multilogue-net-a-context-aware-rnn-for-multi"><a href="#https-paperswithcode-com-paper-multilogue-net-a-context-aware-rnn-for-multi" class="headerlink" title="https://paperswithcode.com/paper/multilogue-net-a-context-aware-rnn-for-multi"></a><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/multimodal-behavioral-markers-exploring">https://paperswithcode.com/paper/multilogue-net-a-context-aware-rnn-for-multi</a></h3><p>这个没看懂，但是星很高。。</p>
<h3 id="The-MuSe-2021-Multimodal-Sentiment-Analysis-Challenge-Sentiment-Emotion-Physiological-Emotion-and-Stress"><a href="#The-MuSe-2021-Multimodal-Sentiment-Analysis-Challenge-Sentiment-Emotion-Physiological-Emotion-and-Stress" class="headerlink" title="The MuSe 2021 Multimodal Sentiment Analysis Challenge: Sentiment, Emotion, Physiological-Emotion, and Stress"></a>The MuSe 2021 Multimodal Sentiment Analysis Challenge: Sentiment, Emotion, Physiological-Emotion, and Stress</h3><p>语言识别的四个挑战</p>
<p>2021年的多模态情感分析(MuSe)是一项挑战，侧重于通过更全面地整合视听、语言和生物信号模式，实现情感和情感任务，以及生理情感和基于情感的压力识别。2021年MuSe的目的是将不同学科的社区聚集在一起;主要是视听情感识别社区(基于信号)、情感分析社区(基于符号)和健康信息学社区。我们提出了四个不同的子挑战:缪斯-怀尔德(MuSe-Wilder)和缪斯-应激(MuSe-Stress)，专注于持续的情绪(效价和唤醒)预测;在MuSe-Sent中，参与者分别识别五种不同的效价和兴奋程度;以及MuSe-Physio，它是用来预测“生理-情感”的新方面。在今年的挑战中，我们使用了专注于用户生成评论的MuSe-CaR数据集，并引入了Ulm-TSST数据集，该数据集显示了人们在压力下的取证。本文还详细介绍了从这些数据集中提取的最先进的特征集，以供我们的基线模型(长短期记忆-循环神经网络)使用。对于每个子挑战，为参与者设定了一个竞争基线;即，在测试中，我们报告了缪斯-怀尔德的一致性相关系数(CCC)为.4616 CCC;MuSe-Stress为。4717 CCC, MuSe-Physio为。4606 CCC。对于MuSe-Sent, F1得分为32.82%。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/lstappen/MuSe2021">https://github.com/lstappen/MuSe2021</a></p>
<h1 id="paperwithcode检索链接"><a href="#paperwithcode检索链接" class="headerlink" title="paperwithcode检索链接"></a>paperwithcode检索链接</h1><p><a target="_blank" rel="noopener" href="https://paperswithcode.com/task/multimodal-sentiment-analysis">https://paperswithcode.com/task/multimodal-sentiment-analysis</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">YAMY</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yamy1234.github.io/2022/10/30/graduate-training-one/">http://yamy1234.github.io/2022/10/30/graduate-training-one/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/deep-learning/">deep_learning</a></div><div class="post_share"><div class="social-share" data-image="/img/5.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/12/13/project-collections-Smart-Vehicle/"><img class="prev-cover" src="/img/2.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">project collections: Smart-Vehicle</div></div></a></div><div class="next-post pull-right"><a href="/2022/05/15/solvePythonWithVscodeServer/"><img class="next-cover" src="/img/2.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">SolvePythonWithVscodeServer</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/head.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">YAMY</div><div class="author-info__description"></div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">18</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">6</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">7</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%AF%95%E4%B8%9A%E5%AE%9E%E8%AE%AD-1"><span class="toc-number">1.</span> <span class="toc-text">毕业实训-1</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AF%86%E7%A0%81"><span class="toc-number">1.1.</span> <span class="toc-text">服务器密码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9B%B8%E5%85%B3%E6%80%A7%E7%A0%94%E7%A9%B6"><span class="toc-number">1.2.</span> <span class="toc-text">多模态相关性研究</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Bi-Bimodal-Modality-Fusion-for-Correlation-Controlled-Multimodal-Sentiment-Analysis"><span class="toc-number">1.2.1.</span> <span class="toc-text">Bi-Bimodal Modality Fusion for Correlation-Controlled Multimodal Sentiment Analysis</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%B9%E4%BA%8EMultimodal-Sentiment-Analysis%E7%9A%84%E6%95%B4%E4%BD%93%E5%88%86%E6%9E%90"><span class="toc-number">1.3.</span> <span class="toc-text">对于Multimodal Sentiment Analysis的整体分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E6%80%81%E8%9E%8D%E5%90%88%E2%80%94%E2%80%94%E4%BA%A4%E4%BA%92%E5%BB%BA%E6%A8%A1"><span class="toc-number">1.4.</span> <span class="toc-text">模态融合——交互建模</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Multimodal-Sentiment-Analysis-with-Word-Level-Fusion-and-Reinforcement-Learning"><span class="toc-number">1.4.1.</span> <span class="toc-text">Multimodal Sentiment Analysis with Word-Level Fusion and Reinforcement Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multimodal-Transformer-for-Unaligned-Multimodal-Language-Sequences"><span class="toc-number">1.4.2.</span> <span class="toc-text">Multimodal Transformer for Unaligned Multimodal Language Sequences</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Improving-Multimodal-Fusion-with-Hierarchical-Mutual-Information-Maximization-for-Multimodal-Sentiment-Analysis"><span class="toc-number">1.4.3.</span> <span class="toc-text">Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cross-Modal-BERT-for-Text-Audio-Sentiment-Analysis"><span class="toc-number">1.4.4.</span> <span class="toc-text">Cross-Modal BERT for Text-Audio Sentiment Analysis</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multimodal-Emotion-Recognition-with-Transformer-Based-Self-Supervised-Feature-Fusion"><span class="toc-number">1.4.5.</span> <span class="toc-text">Multimodal Emotion Recognition with Transformer-Based Self Supervised Feature Fusion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SWAFN-Sentimental-Words-Aware-Fusion-Network-for-Multimodal-Sentiment-Analysis"><span class="toc-number">1.4.6.</span> <span class="toc-text">SWAFN: Sentimental Words Aware Fusion Network for Multimodal Sentiment Analysis</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TEASEL-A-Transformer-Based-Speech-Prefixed-Language-Model"><span class="toc-number">1.4.7.</span> <span class="toc-text">TEASEL: A Transformer-Based Speech-Prefixed Language Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#An-AutoML-based-Approach-to-Multimodal-Image-Sentiment-Analysis"><span class="toc-number">1.4.8.</span> <span class="toc-text">An AutoML-based Approach to Multimodal Image Sentiment Analysis</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MMLatch-Bottom-up-Top-down-Fusion-for-Multimodal-Sentiment-Analysis"><span class="toc-number">1.4.9.</span> <span class="toc-text">MMLatch: Bottom-up Top-down Fusion for Multimodal Sentiment Analysis</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E6%80%81%E8%BD%AC%E5%8C%96%E2%80%94%E2%80%94%E5%85%B6%E4%BB%96%E6%A8%A1%E6%80%81%E5%88%B0%E3%80%90%E8%AF%AD%E8%A8%80%E3%80%91%E6%A8%A1%E6%80%81%E7%9A%84%E8%BF%81%E7%A7%BB-x2F-%E6%98%A0%E5%B0%84-x2F-%E8%BE%85%E5%8A%A9"><span class="toc-number">1.5.</span> <span class="toc-text">模态转化——其他模态到【语言】模态的迁移&#x2F;映射&#x2F;辅助</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Words-Can-Shift-Dynamically-Adjusting-Word-Representations-Using-Nonverbal-Behaviors"><span class="toc-number">1.5.1.</span> <span class="toc-text">Words Can Shift: Dynamically Adjusting Word Representations Using Nonverbal Behaviors</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multimodal-Sentiment-Analysis-using-Hierarchical-Fusion-with-Context-Modeling"><span class="toc-number">1.5.2.</span> <span class="toc-text">Multimodal  Sentiment Analysis using Hierarchical Fusion with Context Modeling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Exploiting-BERT-For-Multimodal-Target-Sentiment-Classification-Through-Input-Space-Translation"><span class="toc-number">1.5.3.</span> <span class="toc-text">Exploiting BERT For Multimodal Target Sentiment Classification Through Input Space Translation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E6%80%81%E5%AF%B9%E6%AF%94%E2%80%94%E2%80%94%E4%BB%A5%E6%A8%A1%E6%80%81%E4%B9%8B%E9%97%B4%E7%9A%84%E7%9B%B8%E4%BC%BC%E6%80%A7vs%E5%B7%AE%E5%BC%82%E6%80%A7%E6%9D%A5%E5%88%86%E6%9E%90%E6%83%85%E6%80%81"><span class="toc-number">1.6.</span> <span class="toc-text">模态对比——以模态之间的相似性vs差异性来分析情态</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MISA-%E5%8F%8C%E6%A8%A1%E6%80%81%EF%BC%9A%E4%B8%A4%E4%B8%AA%E4%B8%8D%E5%90%8C%E7%9A%84%E6%98%A0%E5%B0%84%E7%A9%BA%E9%97%B4"><span class="toc-number">1.6.1.</span> <span class="toc-text">MISA 双模态：两个不同的映射空间</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Learning-Modality-Specific-Representations-with-Self-Supervised-Multi-Task-Learning-for-Multimodal-Sentiment-Analysis"><span class="toc-number">1.6.2.</span> <span class="toc-text">Learning Modality-Specific Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%B6%E5%BA%8F-%E5%AF%B9%E9%BD%90%E5%88%9B%E6%96%B0"><span class="toc-number">1.7.</span> <span class="toc-text">时序-对齐创新</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Deep-HOSeq-Deep-Higher-Order-Sequence-Fusion-for-Multimodal-Sentiment-Analysis"><span class="toc-number">1.7.1.</span> <span class="toc-text">Deep-HOSeq: Deep Higher Order Sequence Fusion for Multimodal Sentiment Analysis</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MTAG-Modal-Temporal-Attention-Graph-for-Unaligned-Human-Multimodal-Language-Sequences"><span class="toc-number">1.7.2.</span> <span class="toc-text">MTAG: Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">1.8.</span> <span class="toc-text">其他</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#M-SENA-%E4%B8%80%E4%B8%AA%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0%EF%BC%9F"><span class="toc-number">1.8.1.</span> <span class="toc-text">M-SENA: 一个多模态情感分析平台？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Integrating-Multimodal-Information-in-Large-Pretrained-Transformers"><span class="toc-number">1.8.2.</span> <span class="toc-text">Integrating Multimodal Information in Large Pretrained Transformers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#https-paperswithcode-com-paper-multilogue-net-a-context-aware-rnn-for-multi"><span class="toc-number">1.8.3.</span> <span class="toc-text">https:&#x2F;&#x2F;paperswithcode.com&#x2F;paper&#x2F;multilogue-net-a-context-aware-rnn-for-multi</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#The-MuSe-2021-Multimodal-Sentiment-Analysis-Challenge-Sentiment-Emotion-Physiological-Emotion-and-Stress"><span class="toc-number">1.8.4.</span> <span class="toc-text">The MuSe 2021 Multimodal Sentiment Analysis Challenge: Sentiment, Emotion, Physiological-Emotion, and Stress</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#paperwithcode%E6%A3%80%E7%B4%A2%E9%93%BE%E6%8E%A5"><span class="toc-number">2.</span> <span class="toc-text">paperwithcode检索链接</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/12/13/about_myself/" title="A Page about Myself"><img src="/img/1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="A Page about Myself"/></a><div class="content"><a class="title" href="/2023/12/13/about_myself/" title="A Page about Myself">A Page about Myself</a><time datetime="2023-12-13T11:13:37.000Z" title="Created 2023-12-13 19:13:37">2023-12-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/13/project-collections-L2TP_VPN_Experiment/" title="project collections: L2TP VPN Experiment"><img src="/img/2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="project collections: L2TP VPN Experiment"/></a><div class="content"><a class="title" href="/2022/12/13/project-collections-L2TP_VPN_Experiment/" title="project collections: L2TP VPN Experiment">project collections: L2TP VPN Experiment</a><time datetime="2022-12-13T04:23:37.000Z" title="Created 2022-12-13 12:23:37">2022-12-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/13/project-collections-RSA_Encript_System/" title="project collections: RSA Encrypt System"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="project collections: RSA Encrypt System"/></a><div class="content"><a class="title" href="/2022/12/13/project-collections-RSA_Encript_System/" title="project collections: RSA Encrypt System">project collections: RSA Encrypt System</a><time datetime="2022-12-13T04:16:37.000Z" title="Created 2022-12-13 12:16:37">2022-12-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/13/project-collections-LinuxPersonalFirewall/" title="project collections: Linux Personal Firewall"><img src="/img/3.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="project collections: Linux Personal Firewall"/></a><div class="content"><a class="title" href="/2022/12/13/project-collections-LinuxPersonalFirewall/" title="project collections: Linux Personal Firewall">project collections: Linux Personal Firewall</a><time datetime="2022-12-13T04:13:37.000Z" title="Created 2022-12-13 12:13:37">2022-12-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/13/project-collections-CARBuddy/" title="project collections: CARBuddy"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="project collections: CARBuddy"/></a><div class="content"><a class="title" href="/2022/12/13/project-collections-CARBuddy/" title="project collections: CARBuddy">project collections: CARBuddy</a><time datetime="2022-12-13T03:35:37.000Z" title="Created 2022-12-13 11:35:37">2022-12-13</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By YAMY</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>